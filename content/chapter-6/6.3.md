---
title: "6.3 Modeling Data"
description: "It is absolutely certain and exact that the proportion between the periodic times of any two planets is precisely the sesquialternate proportion of the mean distances. -Johannes Kepler"
slug:
image: "solar-system-11111_1280.jpg"
draft: true
---
{{% imgcap file="/img/chapter-2/solar-system-11111_1280.jpg" title="Image by NASA" source="https://en.wikipedia.org/wiki/File:Solar_sys8.jpg" %}}

**UNDER REVISION**

Objectives: 
* Utilize regression analysis technology, such as a graphing calculator or software, to find best-fit functions.
* Compare and evaluate different regression models to choose the most appropriate one for a specific data set.
* Use interpolation and extrapolation and address their limitations and potential errors.
* Also discuss realistic domains & ranges

## Introduction
When Johannes Kepler came up with his theory for the orbit of planets in 1618, he was only aware of six planets:  Mercury, Venus, Earth, Mars, Jupiter and Saturn.  Kepler based his theory on measurements taken years earlier by Tycho Brahe who made all of his observations with the naked eye--the telescope was invented 8 years after his death.

Given the technological limitations of the day, is there any chance these 400 year-old theories are still useful today?


x x x x x x x x x x x x x x x x x x x x x
You may have noticed that frequently occurring things are often much smaller than things which occur rarely. For example, small diamonds are common, but large diamonds like the 45.52 carat Hope Diamond are extremely rare. Or the world population of large fish like the whale shark is much smaller than that of tiny fish like sardines.

Craters on the Moon have a similar pattern.  There are very few large craters but millions of small ones.



In the natural sciences, many other aspects of an organism's life are connected to its size. Life expectancy, brain size, gestational period, metabolic rate, just to name a few, depend on body mass. These connections are often called "power laws", because they involve power functions.

With only two data points it's hard to be sure.  Could be exponential.  Logarithms could be used....to check




x x x x x x x x x x x x x x


## Modeling with Technology
{{% imgcap file="/img/chapter-1/horses-61158_1280.jpg" title="Photo Steppinstars from Pixabay " source="https://pixabay.com/images/id-61158/"%}}

We now turn our attention to another scenario.  Throughout the grasslands of Nevada live herds of wild horses known as "mustangs".  Since 1971, the Bureau of Land Management (BLM) has been monitoring the number of mustangs and the land's capacity to support them.  When the number of horses exceeds an appropriate management level, the BLM gathers animals from overpopulated herds and makes them available for adoption.

Every year the BLM conducts aerial surveys in an effort to count each horse.  A functional model of that data can be used to predict the future size of a herd, helping the BML anticipate when it might be necessary to prepare some mustangs for adoption.

The table below gives the total number of mustangs in Nevada since 2000.

![](/img/chapter-1/horse_data.png)
Since the population decreased and then started to increase, it seems reasonable that a transformation of the square function $f(x)=x^{2}$ might fit the data.  

In the figure below the vertex form of a quadratic is shown along with the data.  Adjust the values of $\color{blue}{a}$, $\color{brown}{h}$ and $\color{green}{k}$ until you find a good fit for the data.

{{% geogebra ratio="50%" id_1="NYMIg0eN3w5XIZtN" id_2="FdLFe4II" id_m="FdLFe4II" %}}

With data sets like this one it is hard to know which combination of transformations will make the curve fit the data the best.  In instances like this it is often best to let technology find the equation for us.

Most graphing calculators have a quadratic regression program `QuadReg` that finds a quadratic equation fitting the data.  Here we will show the steps for the Texas-Instruments 83/84 series of calculators.  

We start by entering the data into the calculator by pressing the `STAT` button and selecting `1: Edit`.

![](/img/chapter-1/horse_regression_1.png#center)

The data is entered into the `L1` and `L2` lists.

![](/img/chapter-1/horse_regression_2.png#center)

To view the scatterplot, press `2ND` and `Y=` to enter the `STAT PLOTS` menu,  then select the first plot and make sure it is turned on.

![](/img/chapter-1/horse_regression_3.png#center)

Using `9:ZoomStat` in the `ZOOM` menu gives a good viewing window for the data.

![](/img/chapter-1/horse_regression_4.png#center)

Once we have confirmed the shape of the data, we go back to the `STAT` menu and this time choose the `CALC` tab.

![](/img/chapter-1/horse_regression_5.png#center)

This reveals all of the regression models built into the calculator, letting us choose the model that we think visually matches the shape of the data.  Since this data looks like the square function, we choose `5: QuadReg`.  If the data had looked like a line then `4: LinReg(ax+b)` would have been a good choice.

After running `5: QuadReg` we are shown the equation that fits the data.

![](/img/chapter-1/horse_regression_6.png#center)

The result is given in the standard form $y = a x^{2} + b x + c$ rather than in the vertex form we used when trying to fit the data by hand.  Copying this equation into the graphing window shows that it fits the data reasonably well.

![](/img/chapter-1/horse_regression_7.png#center)

We could now use the equation $y=239.8x^2-3147.9x+24397.8$ as the basis for a model of the wild mustang population in Nevada.

It bears repeating that a mathematical model is not the same as the real object, nor does it control the behavior of the real thing.  For instance, this model might *predict* the future number of wild mustangs, but it is in no way a guarantee of what will happen.


x x x x x x x x x x x x x x 






## Kepler's Third Law
Kepler struggled for 17 years to find a relationship between the time it takes a planet to orbit the Sun (called its orbital period) and its average distance from the Sun (called the semi-major axis).  Eventually he settled on the "sesquialternate proportion", a fancy way to say the $3/2$ power.

By using the `PwrReg` program in a calculator we should be able to do the same thing in just a couple of minutes.

The table below lists the periods and distances for the 8 major planets in our solar system.  

![](/img/chapter-2/planets.svg#center)

We start by entering the data into a calculator

![](/img/chapter-2/kepler_data.svg#center)  

and taking a look at the scatterplot.  Using `9: ZoomStat` gives a good viewing window.

![](/img/chapter-2/kepler_scatterplot.svg#center)

The scatterplot definitely looks like it could be a power function, so we run `PwrReg`

![](/img/chapter-2/kepler_equation.svg#center)

and get back the equation $y=x^{1.5}$.  Graphing this equation along with our scatterplot shows that it is a very good fit.

![](/img/chapter-2/kepler_fit.svg#center)

If we write the power $1.5$ as $3/2$ then we have the same model as Kepler.

> The orbital period $P$ of a planet, in years, can be modeled by the equation
> \[ P =  D^{3/2} \]
> where $D$ is the distance of the planet from the Sun in astronomical units (AU).

Remember that including a description of the variables is an essential part of making a model.  If the data had been measured in days instead of years, or miles instead of astronomical units, then the equation would be different.

{{% check %}}
Kepler discovered his third law by studying planets.  Below are the values for some objects in our solar system that are not planets.  ![](/img/chapter-2/celestial_objects.svg#center)  Does Kepler's 3rd law work for these objects as well?  In other words, do the values fit the equation $P =  D^{3/2}$?{{% answer %}} Yes, Kepler's 3rd law is works for all of these objects, not just planets.{{% /answer %}}
{{% /check %}}


## Allometry
{{% imgcap file="/img/chapter-2/james-youn-IA2pNk_dyqw-unsplash.jpg" title="Photo by James Youn on Unsplash" source="https://unsplash.com/photos/IA2pNk_dyqw" %}}
In the natural sciences, many other aspects of an organism's life are connected to its size. Life expectancy, brain size, gestational period, metabolic rate, just to name a few, all depend on body mass. The study of the relationship of body size to shape, anatomy, physiology and behavior is called "allometry".  Very often allometric data is best modeled by power functions.

For example, the accompanying table shows the weight and cruising speed of several birds.  

![](/img/chapter-2/bird_data.svg#center)

A scatterplot of the data suggests that a slowly increasing power function with $0<p<1$ might model the data well.  

{{% geogebra ratio="33.33%" id_1="ncguvQHLwhXvamtn" id_2="ZnU6AiQr" id_m="ZnU6AiQr" %}}
</br>
Running the `PwrReg` program gives $f(x) =  23.66 x^{0.2}$ as the best fit, confirming our initial observation that $0 < p < 1$.

We can use this function to predict the flying speed of other animals based on their weight.  For instance, the giant golden-crowned flying fox is one of the heaviest bat species, with some individuals weighing up to $3.1$ pounds.  At that weight we would predict an optimal flying speed of $f(3.1) =  23.66 (3.1)^{0.2}=29.67 \text{ mph}$.

{{% check %}}
Check to see if this function works for an airplane like the Cessna 172 Skyhawk, which has a cruising speed of $140 \text{ mph}$ and weighs $2300 \text{ pounds}$.
{{% answer %}}This model predicts a flying speed of \[ f(2300) =  23.66 (2300)^{0.2} = 111.27 \text{ mph} \] which isn't too far off the actual value of $140 \text{ mph}$.  {{% /answer %}}
{{% /check %}}


## Book Sales
In the previous examples, the independent variables were well defined.  However, it is sometimes possible to fit a power function to data even if there is no independent variable.  In such cases, it is common to sort the data from largest to smallest and use the rank as the x-value.  For instance, $x=1$ corresponds to the largest value, $x=2$ corresponds to the second largest value, and so on.

One place to find ranked data is in a best-selling book list.  This table shows some of the best-selling books in the United Kingdom from 1998 though the end of 2011.

![](/img/chapter-2/Top_books_UK.svg#center)

Since the values decrease, if a trend exists it will resemble a decreasing power function, one where the power is negative.  

{{% geogebra ratio="33.33%" id_1="suD06NFpAVH7oCiU" id_2="Wps11VWC" id_m="Wps11VWC" %}}
</br>
A scatterplot of the book data shows a pattern that appears to match the shape of a power function with $p<1$.  Using a power regression program, we find the power function that fits this data best is

\[
  f(x)=5408784.21 x^{-0.46}
\]

Eric Carle's children's book *The Very Hungry Catepillar* is ranked 54th on the list.  According to our model, the number of copies sold should be

\[
  f(54) = 5408784.21(54)^{-0.46}\approx 863373
\]

which is fairly close to the 855,920 actually sold.

{{% check %}}
Use the function above to estimate the ranking of John Grisham's book *The Summons*, which sold 677,378 copies, by solving for $x$ when $f(x)=677378$. {{% answer %}}

\[
  \begin{align}
    677378 &=  5 408 784.21 x^{-0.46} && \text{Set up the equation.} \newline
    0.1252366 &= x^{-0.46} && \text{Divide both sides by 5408784.21.} \newline
    91 &\approx x && \text{Apply the } 1/-0.46 \text{ power to both sides.}
  \end{align}
\]

*The Summons* actually ranked number 84 which is reasonably close to our approximation.
{{% /answer %}}
{{% /check %}}


## Census Data
Another example of ranked data comes to us from the 2010 United States Census, which gives the population of every city in the country.  This table shows the rank and populations of several cities.

![](/img/chapter-2/census_data.svg#center)

A scatterplot of the data shows a clear power function trend modeled by the function $f(x) = 6274804.56x^{-0.74}$.

{{% geogebra ratio="33.33%" id_1="lKfVBqdt3s0cBSli" id_2="W8vbZMNE" id_m="W8vbZMNE" %}}

While this model fits the overall trend of the data very well, we should not expect it to perfectly match every value.

For instance, the Census shows that Kansas City, Missouri was ranked 37th with a population of 459,787.  However, our model predicts a population of $f(37)\approx433,642$.  That's reasonably close, but not perfect.  

This is a common occurrence with many models and we should be cautious about expecting perfection from them.    

{{% check %}}
Follow this [link](http://www.wolframalpha.com/widgets/view.jsp?id=e7f26c273c1aec446e52d436e32e1a49) to check the rank and population of your favorite city.  How does it compare to the value predicted by our model?
{{% answer %}}Your answer will depend on the city chosen.  Here are a few examples.

|  Actual Data from 2010 | Predicted Population |
| --- | --- |
| San Fransico, CA was ranked 13th with a population of 805,235. | $f(13)\approx940,333$ |
| Cleveland, OH was ranked 44th with a population of 396,815. | $f(44)\approx381,456$ |
| Kent, WA was ranked 285th with a population of 92,411. | $f(285)\approx95,723$ |
{{% /answer %}}
{{% /check %}}

x x x x x x x x x x x x x x 

## Fitting Exponential, Logarithmic and Logistic Functions to Data
When provided with data, it is often possible to use a built in regression program to find the equation of a function that fits the data.

{{% imgcap file="/img/chapter-3/39664780224_6dd2c482f6_b.jpg" title="Photo by Jim Choate on Flickr" source="https://www.flickr.com/photos/jimchoate/39664780224/" %}}

For instance, in 1970 the total population of Wheeler county in central Oregon was just 1849 people. By 1990, that number had dropped to 1396 people. Assuming that the population decays exponentially, we can use an exponential regression program like `ExpReg` to find an exponential model for the given data.  

The steps for doing this will vary depending on the calculator used.  You will want to become familiar with the instructions for your calculator.  The basic process should be similar to the steps you used in previous chapters for  `LinReg` , `QuadReg`  and `PwrReg`.

Since the initial population given is from 1970, it makes sense let our variable $x$ be years since 1970.  After entering the data and running `ExpReg`

![](/img/chapter-3/expreg_1.png#center)

we see that the population of Wheeler county Oregon can be modeled by the function $f(x)=1849(0.986)^{x}$, where $x$ is years since 1970.  

{{% check %}}
1.  Using this model, interpolate the population for 1980.  How does this prediction match the actual US Census value of 1513 people?  {{% answer %}}Since 1980 is in between our two data points, we expect that interpolating the value for 1980 should be reasonably close.  With $x = 10$ we get
\[f(10) = 1849(0.986)^{10 \approx = 1606\] which isn't too far off the actual US Census value of 1513.{{% /answer %}}
{{% /check %}}

Let's use our model $f(x) = 1849(0.986)^{x}$, where $x$ is years since 1970, to predict the population of Wheeler county in 2010.  Using $x = 40$ we have

\[
  f(40) = 1849(0.986)^{40} \approx 1052
\]

meaning that our model predicts that the population should have been 1052 people in 2010.  But according to the US Census, Wheeler county actually had 1441 residents in 2010!

Inaccuracies like this are a risk any time we extrapolate our results far into the future, especially if the model is built on limited data.  A more reasonable approach would be to gather all of the available census data and then make a model.  The data and scatterplot below includes US Census data from 1970 through 2010.

![](/img/chapter-3/wheeler_scatterplot.png#center)

It's not clear from the scatterplot that an exponential function would match the shape of the data.  What we seem to need is a function that can go down and up and then down again.  In Chapters 4 and 5 we'll discover polynomial functions which increase and decrease any number of times.  

Even though polynomials will eventually let us find a function that fits this data exactly, it still may not be a good predictor of the future.  This uncertainty is one of the difficulties in modeling real life situations.  

As we have just seen, it is best to include as much data as possible and consider the shape of the data before selecting a model.  We can then compare the shape of the data to the shape of known functions and decide which model to use.  For instance, the three data sets below appear to follow exponential, logarithmic, and logistic trends.

![](/img/chapter-3/scatterplots.svg#center)

Remember, it is easy to mistakenly identify data with a power function shape as having an exponential or logarithmic trend, so it is good practice to try more than one type of function model to see which fits best.

{{% check %}}
The following scatterplots have shapes similar to functions that have been discussed in earlier chapters.  Describe the shape of each one.

1. &nbsp; ![](/img/chapter-1/scatterplot_quadratic.svg#center) {{% answer %}}This data looks similar to a quadratic function.{{% /answer %}}
1. &nbsp; ![](/img/chapter-1/scatterplot_linear.svg#center) {{% answer %}}This data has the shape of a linear function.{{% /answer %}}
1. &nbsp; ![](/img/chapter-1/scatterplot_sqrt.svg#center) {{% answer %}}This data is similar to a power function with a power of $0<p<1$, such as the square root function.  This shape is also visually similar to a logarithmic function.{{% /answer %}}
1. &nbsp; ![](/img/chapter-1/scatterplot_reciprocal.svg#center) {{% answer %}}This data has the shape of a power function with a power of $p<0$, such as the reciprocal function.  This shape is also similar to an exponential decay function.{{% /answer %}}
{{% /check %}}


![](/img/chapter-3/standing_mile.gif#center)

In the summer of 2005, Road and Track magazine gathered 13 cars, 1 motorcycle, and a Navy F/A-18 Hornet fighter jet for a one-mile long acceleration test at the Lemoore Naval Air Base in California.  The first-placed Lola Champ Car (yellow car on the bottom right) crossed the finish line in just 24.2 seconds!  The following table show how long it took the Lola to reach different speeds.

| Time (sec) | Speed (mph) |
| ----- | ----- |
| 3.1 | 60 |
| 5.6 | 100 |
| 8.6 | 150 |
| 9.9 | 161.4 |
| 22 | 200 |
| 24.2 | 203.3 |

Which would fit the data better: an exponential model, a logarithmic model, or a logistic model?  To find out, we start by creating a scatterplot of the data.

![](/img/chapter-3/standing_mile_scatterplot.png#center)

Looking at the scatterplot helps us narrow down the list of possible models.  For instance, the data does not appear to be linear, so no attempt will be made at running `LinReg` and trying to fit a line to the data.

{{% check %}}
1.  Of all the functions you've seen, which one(s) do you think have a shape similar to the shape of this scatterplot?
{{% answer %}}Of the shapes we have seen, this might match a logarithmic function, a logistic function, or even a quadratic.  In a moment we will try each of these and choose the one that appears to fit best.{{% /answer %}}
{{% /check %}}


In an attempt to find a function that models the speed of the Lola Champ Car (bottom right), we have run a natural log regression `LnReg`, a logistic regression `Logistic`, and a quadratic regression `QuadReg`.  The equations and graphs are shown below.

![](/img/chapter-3/standing_mile_regressions.png#center)

It appears that the logistic function $f(x) = \frac{201.91}{1+6.31 e^{-0.334x}}$ is very close to each point, and we choose it as the best fit.  The carrying capacity $201.91$ of the model also seems to match up closely with the top speed $203.3$ of the car, even though it is not an exact match.  Since all models are an approximate fit, we should not expect them replicate every data point exactly.


Let's consider one final example.

The majority of teachers in primary education (K-12) are women. However, positions in K-12 administration, such as principal or superintendent, are more frequently held by men. The following data, from Indiana's department of education, shows the percent of women in K-12 administration. {{% marnote %}}![](/img/chapter-3/woman-2773007_1280.jpg) [Image by ernestoeslava on Pixabay](https://pixabay.com/images/id-2773007/){{% /marnote %}}

| Year | % of women administrators |
| ----- | ----- |
| 1990 | 18.3 |
| 1992 | 22.8 |
| 1994 | 26.7 |
| 1996 | 30.2 |
| 1998 | 32.5 |
| 2000 | 34.4 |
| 2002 | 36.3 |
| 2004 | 37.8 |
| 2006 | 39.3 |

A scatterplot of the data shows an increasing, concave down trend.

![](/img/chapter-3/educator_scatterplot.png#center)

Due to the shape, we can eliminate linear and exponential models from consideration.

{{% check %}}
1.  Based on the shape of the data in the scatterplot, which type of function do you think would be the best fit? {{% answer %}}The shape is similar to a logarithmic function, but it is also reminiscent of a power function with a power $0<p<1$, like the square root function.  It might also be worthwhile to see if a logistic function might fit.{{% /answer %}}
{{% /check %}}


Before running any regressions, we stop to point out that our normal practice letting $x$ represent years since 1990 could be problematic, since $0$ is not in the domain of any logarithmic function.  To eliminate that possibility, we let $x$ represent years since 1989.  Now all of our $x$ values are positive.   

| Years since 1989 | % of women administrators |
| ----- | ----- |
| 1 | 18.3 |
| 3 | 22.8 |
| 5 | 26.7 |
| 7 | 30.2 |
| 9 | 32.5 |
| 11 | 34.4 |
| 13 | 36.3 |
| 15 | 37.8 |
| 17 | 39.3 |

Based on the curve of the data, we chose to run a logistic regression `Logistic`, a logarithmic regression `LnReg`, and a power regression `PwrReg`.  The results are shown below.

![](/img/chapter-3/educator_regressions.png#center)

All three models match the data fairly well.  However, the logistic function has a significant flaw.  Remember that the carrying capacity $c$ is a horizontal asymptote.  In this case, the logistic model would never predict a percent of female administrators that exceeds the carrying capacity of 40.866%.  

Consequently, either the logarithmic or the power models might be better predictors of future trends.

x x x x x x x x x x x x x x 

## Polynomial regression
Polynomials can be very useful when modeling data. Generally, if you notice a pattern of $n$ turns in the data you can model it effectively with an $n-1$ degree polynomial. Most calculators can do up to 4th degree polynomial
regression and programs are available that can do much higher degrees.

{{% imgcap file="/img/chapter-5/6625662063_a45070af3b_b.jpg" title="Photo of Smith Rock by Unsettler on Flickr" source="https://flic.kr/p/b6ufuP" %}}

With stunning mountain views, snowy winters, warm summers and world-class outdoor activities, Redmond is
one of the fastest growing cities in Oregon. The following table shows the population growth since 1970.

\[
  \begin{array}{|c|c|}
    \hline
    \text{Year} & \text{Population} \newline
    \hline
    1970 & 3721  \newline
    \hline
    1980 & 6452 \newline
    \hline
    1990 & 7165 \newline
    \hline
    2000 & 13481 \newline
    \hline
    2010 & 26216 \newline
    \hline
  \end{array}
\]

Before choosing a polynomial model we should plot the data and see what it looks like.  To make it easier, we will use years *since* 1970 rather than the actual year.

![](/img/chapter-5/redmond_regression_1.png#center)

The data appears to increase, slow down, and then increase again, suggesting that a cubic polynomial might be a good fit.  Running `CubicReg` on a calculator

![](/img/chapter-5/redmond_regression_2.png#center)

gives us the cubic polynomial

\[ y=0.70x^3-23.89x^2+393.10x+3818.21 \]

which does fit the data well, coming very close to most of the data points.

But since we had 5 data points, a 4th degree polynomial should fit the data exactly.  Running `QuarticReg` shows that the quartic polynomial

\[ y=-0.028x^4+2.97x^3-79.38x^2+798.16x+3721 \]

is indeed a perfect fit, hitting every single point precisely.

![](/img/chapter-5/redmond_regression_3.png#center)

So the 4th degree polynomial must be a better model than the 3rd degree polynomial, right?  Maybe not.

Notice that our quartic model has a negative leading coefficient.  Because of that the right side of the graph must eventually come down. This makes it a poor predictor for events in the distant future.

![](/img/chapter-5/redmond_regression_4.png#center)

The cubic model may prove to be unreliable as well but at least its values seem within the realm of possibility.


